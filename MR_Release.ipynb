{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfb4fcb7-1bf5-45bd-b472-d780ec34cf44",
   "metadata": {},
   "source": [
    "# Model to classify audio data into notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67717cba-0a28-42e4-95cb-77d577727ab2",
   "metadata": {},
   "source": [
    "## 1) Data acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073eaf30-bd9d-4d01-bbd7-a867b738d88d",
   "metadata": {},
   "source": [
    "#### Check sound devices availible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce48f35a-2fe0-40e8-9aaf-06013d7814ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sd.default.device = \"pipewire\"\n",
    "# print(sd.default.device)\n",
    "print(sd.query_devices(),\"\\n\")\n",
    "# print(sd.query_hostapis())\n",
    "\n",
    "device = sd.query_devices(device=\"pipewire\", kind=None)[\"index\"]\n",
    "print(\"Chosen device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629397a2-6097-442f-a0cb-4ec18fc71f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "samplerates = 16000, 32000, 44100, 48000, 96000, 128000\n",
    "\n",
    "supported_samplerates = []\n",
    "for fs in samplerates:\n",
    "    try:\n",
    "        sd.check_output_settings(device=device, samplerate=fs)\n",
    "    except Exception as e:\n",
    "        print(fs, e)\n",
    "    else:\n",
    "        supported_samplerates.append(fs)\n",
    "print(supported_samplerates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a85a41-e454-48e1-8291-0e81a7459be6",
   "metadata": {},
   "source": [
    "#### Custom oscilloscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2304cb5d-4d33-4ce1-b79a-827054b8bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.fft import fft\n",
    "from scipy.signal import stft\n",
    "from scipy.signal import welch\n",
    "from scipy.signal import resample\n",
    "from scipy.signal import square\n",
    "from scipy.signal import butter, freqs, sosfilt\n",
    "\n",
    "class oscope:\n",
    "    def __init__(self, seg, nol, nff):\n",
    "        self.seg = seg\n",
    "        self.nol = nol\n",
    "        self.nff = nff\n",
    "        \n",
    "    def setp(self, seg, nol, nff):\n",
    "        self.seg = seg\n",
    "        self.nol = nol\n",
    "        self.nff = nff\n",
    "        \n",
    "    def tplot(self, t, sig, N):\n",
    "        plt.figure(figsize = (6,3))\n",
    "        plt.plot(t[:N], sig[:N])\n",
    "        plt.title(\"Oscillogram\")\n",
    "        plt.xlabel(\"Time, s\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "    def splot(self, f, fts, N):\n",
    "        plt.figure(figsize = (6,3))\n",
    "        plt.plot(f[:N], fts[:N], color = 'purple')\n",
    "        plt.title(\"Spectrum\")\n",
    "        plt.xlabel(\"Frequency, Hz\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    \n",
    "    def tsplot(self, sig, Fs):\n",
    "        f,fts = welch(sig, window = 'boxcar', fs = Fs, nperseg = self.seg, \n",
    "                      noverlap = self.nol, nfft = self.nff, scaling = \"density\")\n",
    "        fts = np.sqrt(fts*(Fs*2/seg))\n",
    "        self.splot(f, fts, -1)\n",
    "        \n",
    "    def getmax(self, sig, Fs):\n",
    "        f,fts = welch(sig, window = 'boxcar', fs = Fs, nperseg = self.seg, \n",
    "                      noverlap = self.nol, nfft = self.nff, scaling = \"density\")\n",
    "        return f[np.argmax(fts)]\n",
    "\n",
    "    def getfft(self, sig, Fs):\n",
    "        f,fts = welch(sig, window = 'boxcar', fs = Fs, nperseg = self.seg, \n",
    "                      noverlap = self.nol, nfft = self.nff, scaling = \"density\")\n",
    "        return f, fts\n",
    "\n",
    "l = 10000\n",
    "\n",
    "seg = int(l/2)-1 # 4999\n",
    "nol = 0\n",
    "nff = None\n",
    "osc = oscope(seg, nol, nff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10d1faf-fd25-4b5e-9990-cb25fd2c5dc5",
   "metadata": {},
   "source": [
    "### Noise recorder\n",
    "Records all the noise files in one go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e658641d-9dfe-4d78-af78-9d0b07d9484d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 124\u001b[0m\n\u001b[1;32m    122\u001b[0m n_recs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m348\u001b[39m\n\u001b[1;32m    123\u001b[0m duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.5\u001b[39m \n\u001b[0;32m--> 124\u001b[0m \u001b[43mrecord_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmypath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_recs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 98\u001b[0m, in \u001b[0;36mrecord_data\u001b[0;34m(folder_path, duration, n_recordings)\u001b[0m\n\u001b[1;32m     96\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(folder_path)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     proceed_choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mStart a recording?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_recordings): \n\u001b[1;32m    100\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Start a recording? 0\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import sounddevice as sd\n",
    "import wave\n",
    "from scipy.io.wavfile import read\n",
    "from scipy.signal import spectrogram\n",
    "\n",
    "import time\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from IPython.display import clear_output\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = sd.query_devices(device=\"pipewire\", kind=None)[\"index\"]\n",
    "\n",
    "\n",
    "FORMAT = pyaudio.paInt16  # 16 bit mono\n",
    "CHANNELS = 1\n",
    "RATE = 48000  # discretization frequency\n",
    "CHUNK_IN = 1024\n",
    "CHUNK_OUT = 1024\n",
    "DEVICE_IDS = (device, device)\n",
    "\n",
    "e = 10**(-10)\n",
    "\n",
    "def to_wav(path, frames, p, form=FORMAT, ch=CHANNELS, Fs=RATE):\n",
    "    wf = wave.open(path, \"wb\")\n",
    "    wf.setsampwidth(p.get_sample_size(form))\n",
    "    wf.setnchannels(ch)\n",
    "    wf.setframerate(Fs)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "    print(f\"Saved to {path}\")\n",
    "\n",
    "def record(stream, duration, chunk_in = CHUNK_IN, Fs=RATE):\n",
    "    frames = []\n",
    "    stream.start_stream()\n",
    "    for i in range(int(Fs / chunk_in * duration)):\n",
    "        data = stream.read(chunk_in)\n",
    "        frames.append(data)\n",
    "    stream.stop_stream()\n",
    "    return frames\n",
    "    \n",
    "def play(stream, file_path, chunk_out=CHUNK_OUT):\n",
    "    wf = wave.open(file_path, 'rb')\n",
    "    # read data (based on the chunk size)\n",
    "    data = wf.readframes(chunk_out)\n",
    "    # play stream (looping from beginning of file to the end)\n",
    "    stream.start_stream()\n",
    "    while data:\n",
    "        # writing to the stream is what *actually* plays the sound.\n",
    "        stream.write(data)\n",
    "        data = wf.readframes(chunk_out)\n",
    "\n",
    "    stream.stop_stream()\n",
    "    # stream.stop_stream()\n",
    "    wf.close()\n",
    "    \n",
    "def setup_streams(form=FORMAT, chan=CHANNELS, \n",
    "                  chunk_in = CHUNK_IN, Fs=RATE,\n",
    "                  chunk_out=CHUNK_OUT, dev_id = DEVICE_IDS):\n",
    "    \n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    i_stream = p.open(format=form,\n",
    "                channels=chan,\n",
    "                rate=Fs,\n",
    "                input=True,\n",
    "                frames_per_buffer=chunk_in,\n",
    "                input_device_index=dev_id[0])\n",
    "    # open stream based on the wave object which has been input.\n",
    "    o_stream = p.open(format = form,\n",
    "                    channels = chan,\n",
    "                    rate = Fs,\n",
    "                    output = True,\n",
    "                    frames_per_buffer=chunk_out,\n",
    "                    output_device_index=dev_id[1])\n",
    "    i_stream.stop_stream()\n",
    "    o_stream.stop_stream()\n",
    "    return p, i_stream, o_stream\n",
    "\n",
    "def close_streams(p, i_stream, o_stream):\n",
    "    i_stream.close()\n",
    "    o_stream.close()\n",
    "    p.terminate()\n",
    "    print(\"All streams closed\")\n",
    "    \n",
    "    \n",
    "def record_data(folder_path, duration=5, n_recordings=100):\n",
    "    pa, i_str, o_str = setup_streams()\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    try:\n",
    "        proceed_choice = bool(int(input(\"Start a recording?\")))\n",
    "        for i in range(n_recordings): \n",
    "            time.sleep(1)\n",
    "            clear_output()\n",
    "            \n",
    "            frames = record(i_str, duration)\n",
    "            label = \"0\" \n",
    "            cur_file_id = max([int(f.split(\".\")[0].split(\"_\")[1]) \n",
    "                               for f in listdir(folder_path) \n",
    "                               if f.split(\"_\")[0]==label \n",
    "                               and (isfile(join(folder_path, f)))]+[0])+1\n",
    "            print(cur_file_id)\n",
    "            file_name = f\"{label}_{cur_file_id}.wav\"\n",
    "                \n",
    "            path = join(folder_path, file_name)\n",
    "            to_wav(path, frames, pa)\n",
    "            # print(\"Saved\")\n",
    "        close_streams(pa, i_str, o_str)\n",
    "        \n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        close_streams(pa, i_str, o_str)\n",
    "        \n",
    "mypath = \"Data/Notes/New/Noise/\"\n",
    "n_recs = 348\n",
    "duration = 1.5 \n",
    "record_data(mypath, duration, n_recs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1afcbd-f26e-45aa-9090-c8e1a5a96dce",
   "metadata": {},
   "source": [
    "### Notes Recorder\n",
    "Iterative recorder with human interaction needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f1aff68-6780-40a4-8e1f-56db9c49c9c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'osc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m     stream\u001b[38;5;241m.\u001b[39mstop_stream()\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m frames\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvisualize\u001b[39m(file_path, Fs\u001b[38;5;241m=\u001b[39mRATE, os\u001b[38;5;241m=\u001b[39m\u001b[43mosc\u001b[49m):\n\u001b[1;32m     47\u001b[0m     file \u001b[38;5;241m=\u001b[39m read(file_path)\n\u001b[1;32m     48\u001b[0m     sequence \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'osc' is not defined"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import sounddevice as sd\n",
    "import wave\n",
    "from scipy.io.wavfile import read\n",
    "from scipy.signal import spectrogram\n",
    "\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from IPython.display import clear_output\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = sd.query_devices(device=\"pipewire\", kind=None)[\"index\"]\n",
    "\n",
    "FORMAT = pyaudio.paInt16  # 16 bit mono\n",
    "CHANNELS = 1\n",
    "RATE = 48000  # discretization frequency\n",
    "CHUNK_IN = 1024\n",
    "CHUNK_OUT = 1024\n",
    "DEVICE_IDS = (device, device)\n",
    "\n",
    "e = 10**(-10)\n",
    "\n",
    "def to_wav(path, frames, p, form=FORMAT, ch=CHANNELS, Fs=RATE):\n",
    "    wf = wave.open(path, \"wb\")\n",
    "    wf.setsampwidth(p.get_sample_size(form))\n",
    "    wf.setnchannels(ch)\n",
    "    wf.setframerate(Fs)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "    print(f\"Saved to {path}\")\n",
    "\n",
    "def record(stream, duration, chunk_in = CHUNK_IN, Fs=RATE):\n",
    "    frames = []\n",
    "    stream.start_stream()\n",
    "    for i in range(int(Fs / chunk_in * duration)):\n",
    "        data = stream.read(chunk_in)\n",
    "        frames.append(data)\n",
    "    stream.stop_stream()\n",
    "    return frames\n",
    "\n",
    "def visualize(file_path, Fs=RATE, os=osc):\n",
    "    file = read(file_path)\n",
    "    sequence = file[1]\n",
    "    os.tsplot(sequence, Fs)\n",
    "    print(os.getmax(sequence, Fs))\n",
    "    \n",
    "def play(stream, file_path, chunk_out=CHUNK_OUT):\n",
    "    wf = wave.open(file_path, 'rb')\n",
    "    # read data (based on the chunk size)\n",
    "    data = wf.readframes(chunk_out)\n",
    "    # play stream (looping from beginning of file to the end)\n",
    "    stream.start_stream()\n",
    "    while data:\n",
    "        # writing to the stream is what *actually* plays the sound.\n",
    "        stream.write(data)\n",
    "        data = wf.readframes(chunk_out)\n",
    "\n",
    "    stream.stop_stream()\n",
    "    # stream.stop_stream()\n",
    "    wf.close()\n",
    "    \n",
    "def setup_streams(form=FORMAT, chan=CHANNELS, \n",
    "                  chunk_in = CHUNK_IN, Fs=RATE,\n",
    "                  chunk_out=CHUNK_OUT, dev_id = DEVICE_IDS):\n",
    "    \n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    i_stream = p.open(format=form,\n",
    "                channels=chan,\n",
    "                rate=Fs,\n",
    "                input=True,\n",
    "                frames_per_buffer=chunk_in,\n",
    "                input_device_index=dev_id[0])\n",
    "    \n",
    "    o_stream = p.open(format = form,\n",
    "                    channels = chan,\n",
    "                    rate = Fs,\n",
    "                    output = True,\n",
    "                    frames_per_buffer=chunk_out,\n",
    "                    output_device_index=dev_id[1])\n",
    "    i_stream.stop_stream()\n",
    "    o_stream.stop_stream()\n",
    "    return p, i_stream, o_stream\n",
    "\n",
    "def close_streams(p, i_stream, o_stream):\n",
    "    i_stream.close()\n",
    "    o_stream.close()\n",
    "    p.terminate()\n",
    "    print(\"All streams closed\")\n",
    "    \n",
    "    \n",
    "def record_data(folder_path, duration=5):\n",
    "    pa, i_str, o_str = setup_streams()\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    try:\n",
    "        while True: \n",
    "            proceed_choice = bool(int(input(\"Start a recording?\")))\n",
    "            time.sleep(1)\n",
    "            if not proceed_choice:\n",
    "                break\n",
    "            \n",
    "            frames = record(i_str, duration)\n",
    "            tpath = join(folder_path, \"temp.wav\")\n",
    "            to_wav(tpath, frames, pa)\n",
    "            visualize(tpath)\n",
    "            \n",
    "            play(o_str, tpath)\n",
    "    \n",
    "            label = input(\"Enter class label (0-88)\")\n",
    "            \n",
    "            if (not label.isdigit()) or int(label) < 0:\n",
    "                clear_output()\n",
    "                print(\"Not saved\")\n",
    "                continue\n",
    "                \n",
    "            cur_file_id = max([int(f.split(\".\")[0].split(\"_\")[1]) \n",
    "                               for f in listdir(folder_path) \n",
    "                               if f.split(\"_\")[0]==label \n",
    "                               and (isfile(join(folder_path, f)))]+[0])+1\n",
    "    \n",
    "            file_name = f\"{label}_{cur_file_id}.wav\"\n",
    "                \n",
    "            path = join(folder_path, file_name)\n",
    "            to_wav(path, frames, pa)\n",
    "            \n",
    "            clear_output()\n",
    "            print(\"Saved\")\n",
    "        close_streams(pa, i_str, o_str)\n",
    "        \n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        close_streams(pa, i_str, o_str)\n",
    "        \n",
    "mypath = \"Data/Test/Temp/\"\n",
    "duration = 1.5 \n",
    "record_data(mypath, duration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5028fd-9cc0-40a8-b44a-5f085e2587dc",
   "metadata": {},
   "source": [
    "### Dataset transformation\n",
    "Split into chunks (50 ms or 2400 values each), apply FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7277e8c-82c4-46dc-9875-e72f7d18b722",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import shutil\n",
    "import wave\n",
    "from scipy.io.wavfile import read\n",
    "from scipy.signal import welch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class WavRW():\n",
    "    def __init__(self, path):\n",
    "        wf = wave.open(path, \"rb\")\n",
    "        self.form = wf.getsampwidth()\n",
    "        self.ch = wf.getnchannels()\n",
    "        self.Fs = wf.getframerate()\n",
    "        wf.close()\n",
    "        \n",
    "    def save_wav(self, path, frames):\n",
    "        wf = wave.open(path, \"wb\")\n",
    "        wf.setsampwidth(self.form)\n",
    "        wf.setnchannels(self.ch)\n",
    "        wf.setframerate(self.Fs)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "        wf.close()\n",
    "        # print(f\"Saved to {path}\")\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_dir, binary=False, recreate = False, rec_len=5000, step=5000, transform=None, target_transform=None):\n",
    "        self.path = audio_dir\n",
    "        self.rec_len = rec_len\n",
    "        self.step = step\n",
    "        self.audio_dir = audio_dir\n",
    "        self.d1 = \"DataSet/\"\n",
    "        self.d2 = \"FFT/\"\n",
    "        self.d3 = \"Sequences/\"\n",
    "        self.work_dir = self.audio_dir + self.d1\n",
    "        self.di_name = \"data_info.csv\"\n",
    "        self.binary = binary\n",
    "\n",
    "        self.make_dirs()\n",
    "        \n",
    "        if recreate:\n",
    "            self.form_dataset()\n",
    "        else:\n",
    "            self.load_ds_info()\n",
    "        # self.scale_data()\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def make_dirs(self):\n",
    "        new_directories = [self.work_dir, self.work_dir+self.d2, self.work_dir+self.d3]\n",
    "        for dir_path in new_directories:\n",
    "            if os.path.exists(dir_path):\n",
    "                shutil.rmtree(dir_path)\n",
    "            os.makedirs(dir_path)\n",
    "    \n",
    "    # not used  \n",
    "    def scale_data(self):\n",
    "        self.scaler = RobustScaler(copy=False)\n",
    "        self.data_x = self.scaler.fit_transform(data_x)\n",
    "\n",
    "    def get_fft(self, sig, Fs):\n",
    "        nps = len(sig)-1 # int(len(sig)/2-1)\n",
    "        f,fts = welch(sig, window = \"boxcar\", fs = Fs, nperseg = nps, \n",
    "                      noverlap = None, nfft = nps, scaling = \"density\") # here is the scaling\n",
    "        return f, fts\n",
    "\n",
    "    def load_ds_info():\n",
    "        self.data_info = pd.readcsv(self.work_dir+self.di_name)\n",
    "        self.n_classes = len(self.data_info[\"class\"].uniques())\n",
    "        \n",
    "    def form_dataset(self, tr_r=0.8):\n",
    "        file_names = os.listdir(self.path)\n",
    "        file_names = [file_name for file_name in file_names if isfile(self.audio_dir+file_name)]\n",
    "        self.w_rw = WavRW(self.audio_dir+file_names[0])\n",
    "        \n",
    "        if self.binary:\n",
    "            classes = (0, 1)\n",
    "        else:\n",
    "            classes = set([int(file_name.split(\"_\")[0]) for file_name in file_names])\n",
    "        self.n_classes = len(classes)\n",
    "\n",
    "        self.data_info = pd.DataFrame(columns=[\"file_path\", \"fft_path\", \"seq_path\", \"class\", \"cw_id\"])\n",
    "        \n",
    "        iterator = 0\n",
    "        for file_name in file_names:\n",
    "            print(\"Processing file\", file_name)\n",
    "            file_path = self.audio_dir+file_name\n",
    "            wfile = read(file_path)\n",
    "            sequence = wfile[1]\n",
    "            l = len(sequence)\n",
    "            st = 0\n",
    "            # print(math.floor((l-sample_size)/step))\n",
    "            y = int(file_name.split(\"_\")[0])\n",
    "            cw_id = int(file_name.split(\".\")[0].split(\"_\")[1])\n",
    "           \n",
    "            for i in range(math.floor((l-self.rec_len)/self.step)): # step\n",
    "                subseq = sequence[st:st+self.rec_len]\n",
    "                _, x = self.get_fft(subseq, self.w_rw.Fs)\n",
    "                fft_fpath = f\"{self.work_dir}{self.d2}{iterator}.txt\" \n",
    "                np.savetxt(fft_fpath, x) # np.array(subs).astype(np.double)\n",
    "                \n",
    "                seq_fpath = f\"{self.work_dir}{self.d3}{iterator}.wav\" \n",
    "                self.w_rw.save_wav(seq_fpath, subseq)\n",
    "                \n",
    "                self.data_info.loc[iterator] = (file_path, fft_fpath, seq_fpath, y, cw_id)\n",
    "                st += self.step\n",
    "                iterator += 1\n",
    "                \n",
    "            di_fpath = self.work_dir+self.di_name\n",
    "            self.data_info.to_csv(di_fpath, index=True)  \n",
    "\n",
    "        self.fft_len = len(x) #int(self.rec_len/4)\n",
    "        return self.data_info\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data_info.loc[idx]\n",
    "        x = np.loadtxt(row[\"fft_path\"])\n",
    "        \n",
    "        if self.binary:\n",
    "            y = row[\"class\"] > 1\n",
    "            return x, y\n",
    "        else:\n",
    "            y_ohe = np.zeros(self.n_classes)\n",
    "            y_ohe[row[\"class\"]] = 1\n",
    "        \n",
    "        return x, y_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884110d4-8423-42a8-a4ce-480d3be2c574",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"./Data/Notes/Noise/\"\n",
    "r_len = 2400\n",
    "step =  r_len # int(r_len/2)\n",
    "# sample_rate = 48000\n",
    "dsb = AudioDataset(data_path, rec_len=r_len, step=r_len, binary=True, recreate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e43d01-1763-41e3-ace6-02cea27f4a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"./Data/Notes/FullX4/\"\n",
    "r_len = 2400\n",
    "step =  r_len # int(r_len/2)\n",
    "# sample_rate = 48000\n",
    "ds = AudioDataset(data_path, rec_len=r_len, step=r_len, binary=False, recreate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db90e644-3e82-4f9d-ab2e-2e88e837919d",
   "metadata": {},
   "source": [
    "## Classifier and training procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874c0e0d-a16c-4ac0-8b07-2f9bd66e3534",
   "metadata": {},
   "source": [
    "Learning is presented through cross-validation\n",
    "\n",
    "As for classifiers architecture - it's just a simple MLP (with some tunings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dfd6d7c-e54e-4e21-a60b-0c1f67e96c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "class ExperimentsLogger:\n",
    "    def __init__(self, models_path):\n",
    "        self.models_path = models_path\n",
    "        self.cur_id = 0\n",
    "        self.read_experiments()\n",
    "        \n",
    "    def read_experiments(self):\n",
    "        if not os.path.exists(self.models_path):\n",
    "            os.makedirs(self.models_path)\n",
    "        \n",
    "        try:\n",
    "            with open(self.models_path+\"model_experiments.json\", \"r\") as file:\n",
    "                self.experiments = json.load(file)\n",
    "                self.cur_id = int(list(self.experiments.keys())[-1]) + 1\n",
    "                print(\"Current experiment:\", self.cur_id)\n",
    "                \n",
    "        except:\n",
    "            self.experiments = {}\n",
    "            with open(self.models_path+\"model_experiments.json\", \"w\") as file:\n",
    "                json.dump(self.experiments, file)\n",
    "                print(\"No experiments found, creating the new log file\")\n",
    "\n",
    "    def write_experiment(self, exp_info, save=True):\n",
    "        score = exp_info[\"score\"][\"metric\"][\"values\"]\n",
    "        model_name = exp_info[\"model_info\"][\"name\"]\n",
    "        model_path = exp_info[\"model_info\"][\"path\"]\n",
    "        description = exp_info[\"description\"]\n",
    "\n",
    "        final_score = np.mean(score)\n",
    "        self.experiments[self.cur_id] = {\"M\":model_name, \"S\": str(final_score),  \"D\":description, \"P\":model_path, \"E\":exp_info}\n",
    "        \n",
    "        print(f\"\\nAvg cross-val metric: {np.mean(score)}, STD: {np.std(score)}\")\n",
    "        if save:\n",
    "         with open(self.models_path+\"model_experiments.json\", \"w\") as file:\n",
    "             json.dump(self.experiments, file)\n",
    "             print(\"Experiment was written successfully\")\n",
    "        self.cur_id+=1\n",
    "        \n",
    "    def describe_experiments(self):\n",
    "        to_display = [\"P\", \"S\", \"D\"]\n",
    "        print([exp[to_display] for exp in self.experiments.values()], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42039a66-faf0-4f94-afd2-779aa79ba401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Availible GPUs:\n",
      "0 NVIDIA GeForce GTX 1050 Ti\n",
      "\n",
      "Using NVIDIA GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Availible GPUs:\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(i, torch.cuda.get_device_properties(i).name)\n",
    "print()\n",
    "\n",
    "device = \"\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(f\"Using {torch.cuda.get_device_name(device)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbcc68c-c629-41be-85f1-06f3feefafa2",
   "metadata": {},
   "source": [
    "### 1. Noise classifier\n",
    "To determine if a note is really playing atm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe1fdebd-c4ba-4cb6-b69b-303c5426f771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment: 2\n"
     ]
    }
   ],
   "source": [
    "models_path = \"./Models/Noise_BClassifiers/\"\n",
    "elb = ExperimentsLogger(models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe010bf1-abd9-4db9-ae6b-2ba8cb4558c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Epoch: 0 Loss clf: 0.4033673405647278 Metrics: (0.7546225786209106, 0.8649949431419373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-11 (_pin_memory_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mephodius/.pyenv/versions/3.10.6/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/mephodius/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/mephodius/.pyenv/versions/3.10.6/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mephodius/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 59, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/home/mephodius/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 35, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/mephodius/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/mephodius/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages/torch/multiprocessing/reductions.py\", line 541, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/mephodius/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/mephodius/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/mephodius/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/connection.py\", line 513, in Client\n",
      "    answer_challenge(c, authkey)\n",
      "  File \"/home/mephodius/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/connection.py\", line 762, in answer_challenge\n",
      "    response = connection.recv_bytes(256)        # reject large message\n",
      "  File \"/home/mephodius/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/connection.py\", line 221, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/mephodius/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/mephodius/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 98\u001b[0m\n\u001b[1;32m     95\u001b[0m     tr_metric\u001b[38;5;241m.\u001b[39mupdate(prediction\u001b[38;5;241m.\u001b[39mflatten(), y\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m     97\u001b[0m clf\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(testloader):\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    100\u001b[0m         x \u001b[38;5;241m=\u001b[39m x[:,\u001b[38;5;28;01mNone\u001b[39;00m,:]\n",
      "File \u001b[0;32m~/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1402\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1401\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1402\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1403\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1404\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1246\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "from torcheval.metrics import BinaryAccuracy\n",
    "from torch import nn\n",
    "\n",
    "rs = 42\n",
    "torch.manual_seed(rs)\n",
    "np.random.seed(rs)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, n_input):\n",
    "        self.n_neurons = 256\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(n_input, self.n_neurons),# new 128 32\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm1d(self.n_neurons),\n",
    "            nn.Linear(self.n_neurons, 1),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Sigmoid())\n",
    "        # self.model.apply(self.init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    # def validate(self, validation_set, batch_size, loss_function):\n",
    "    #     #  creating a list to hold loss per batch\n",
    "    #     loss_per_batch = []\n",
    "        \n",
    "    #     #  defining model state\n",
    "    #     network.eval()\n",
    "        \n",
    "    #     #  defining dataloader\n",
    "    #     val_loader = DataLoader(validation_set, batch_size)\n",
    "\n",
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "batch_size = 128\n",
    "batch_n = int(dsb.__len__()/batch_size)\n",
    "\n",
    "lr = 0.001\n",
    "num_epochs = 50 # 160 80 # 20\n",
    "loss_function = nn.BCELoss()\n",
    "tr_metric = BinaryAccuracy()\n",
    "val_metric = BinaryAccuracy()\n",
    "mclist = []\n",
    "losslist = []\n",
    "\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dsb)):\n",
    "    torch.cuda.device(1)\n",
    "    # Print\n",
    "    print(f'\\nFOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                      dsb, \n",
    "                      batch_size=batch_size, sampler=train_subsampler, num_workers=2, pin_memory=True)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "                      dsb,\n",
    "                      batch_size=batch_size, sampler=test_subsampler, num_workers=2, pin_memory=True)\n",
    "\n",
    "    clf = Classifier(dsb.fft_len).cuda()\n",
    "    optimizer = torch.optim.Adam(clf.parameters(), lr=lr)\n",
    "    torch.compile(clf)\n",
    "    for epoch in range(num_epochs):\n",
    "        clf.train()\n",
    "        tr_metric.reset()\n",
    "        val_metric.reset()\n",
    "        for n, (x, y) in enumerate(trainloader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "            x = x[:,None,:]\n",
    "            y = y[:,None]\n",
    "            x = x.to(torch.float32).cuda()\n",
    "            y = y.to(torch.float32).cuda()\n",
    "            \n",
    "            prediction = clf(x) \n",
    "            loss_classifier = loss_function(prediction, y)\n",
    "            loss_classifier.backward()\n",
    "            optimizer.step()\n",
    "            tr_metric.update(prediction.flatten(), y.flatten())\n",
    "            \n",
    "        clf.eval()\n",
    "        for n, (x, y) in enumerate(testloader):\n",
    "            with torch.no_grad():\n",
    "                x = x[:,None,:]\n",
    "                y = y[:,None]\n",
    "                x = x.to(torch.float32).cuda()\n",
    "                y = y.to(torch.float32).cuda()\n",
    "                \n",
    "                prediction = clf(x)\n",
    "                val_metric.update(prediction.flatten(), y.flatten())\n",
    "                # print(torch.sum(torch.abs((prediction - y.cuda())))/len(prediction.flatten()))\n",
    "                # metric.update(prediction, y)\n",
    "                # print((prediction).argmax(axis=1)-y.cuda().argmax(axis=1))\n",
    "        mcv = val_metric.compute()\n",
    "         \n",
    "        if epoch % 5 == 0 or epoch == (num_epochs-1):\n",
    "            mct = tr_metric.compute()\n",
    "            print(f\"Epoch: {epoch} Loss clf: {loss_classifier} Metrics: ({mct}, {mcv})\")\n",
    "    \n",
    "    mclist.append(float(mcv))\n",
    "    losslist.append(float(loss_classifier))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bc175a86-1eee-40ba-abde-bbceba98ddf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as ./Models/Noise_BClassifiers/nn_bin-acc_0.9658.pt\n",
      "{'score': {'metric': {'name': 'bin-acc', 'values': [0.9734042286872864, 0.9726443886756897, 0.9721378087997437, 0.9658054709434509, 0.9658054709434509]}, 'loss': {'name': 'bin-cross-entropy', 'values': [0.2739558517932892, 0.2659973204135895, 0.17677325010299683, 0.18252500891685486, 0.11571940034627914]}}, 'data': {'fs': 48000, 'fft_len': 1200, 'duration': 0.05}, 'learning': {'n_epochs': 50, 'n_batches': 128, 'learning_rate': 0.001, 'folds': 5}, 'model_info': {'name': 'nn', 'path': './Models/Noise_BClassifiers/nn_bin-acc_0.9658.pt'}, 'description': 'fft_len 1200, batch 128, 256 neurons, batch_norm before nonlinearity'}\n",
      "\n",
      "Avg cross-val metric: 0.9699594736099243, STD: 0.0034156032251632185\n",
      "Experiment was written successfully\n"
     ]
    }
   ],
   "source": [
    "# saving the model and experiment results\n",
    "\n",
    "metric_name = \"bin-acc\"\n",
    "loss_name = \"bin-cross-entropy\"\n",
    "model_name = \"nn\"\n",
    "description = f\"fft_len {dsb.fft_len}, batch {batch_size}, {clf.n_neurons} neurons, batch_norm before nonlinearity\"\n",
    "\n",
    "\n",
    "model_path = f\"{models_path}{model_name}_{metric_name}_{mclist[-1]:.4f}.pt\"\n",
    "model_scripted = torch.jit.script(clf) # Export to TorchScript\n",
    "model_scripted.save(model_path) # Save\n",
    "\n",
    "print(f\"Saved as {model_path}\")\n",
    "\n",
    "exp_info = {\"score\":{\"metric\":{\"name\":metric_name, \"values\":mclist}, \"loss\":{\"name\":loss_name, \"values\":losslist}},\n",
    "            \"data\": {\"fs\": dsb.w_rw.Fs, \"fft_len\":dsb.fft_len, \"duration\":(dsb.fft_len*2)/dsb.w_rw.Fs},\n",
    "            \"learning\":{\"n_epochs\":num_epochs, \"n_batches\":batch_size, \"learning_rate\":lr, \"folds\":k_folds},\n",
    "            \"model_info\":{\"name\":model_name, \"path\":model_path},\n",
    "            \"description\": description}\n",
    "\n",
    "print(exp_info)\n",
    "elb.write_experiment(exp_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d477a9-4e29-4dbc-a395-2aa7ad376784",
   "metadata": {},
   "source": [
    "#### Get description of the best experiments so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec9c80c9-68df-493e-af44-d8ede9b72013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0', {'M': 'nn', 'S': '0.9733029484748841', 'D': 'fft_len 1200, batch 32, 256 neurons, batch_norm before nonlinearity', 'P': './Models/Noise_BClassifiers/nn_bin-acc_0.9749.pt', 'E': {'score': {'metric': {'name': 'bin-acc', 'values': [0.968845009803772, 0.9739108681678772, 0.9759371876716614, 0.9728976488113403, 0.9749240279197693]}, 'loss': {'name': 'bin-cross-entropy', 'values': [0.22502265870571136, 0.2225693166255951, 0.13281896710395813, 0.30771803855895996, 0.17659813165664673]}}, 'data': {'fs': 48000, 'fft_len': 1200, 'duration': 0.05}, 'learning': {'n_epochs': 50, 'n_batches': 32, 'learning_rate': 0.001, 'folds': 5}, 'model_info': {'name': 'nn', 'path': './Models/Noise_BClassifiers/nn_bin-acc_0.9749.pt'}, 'description': 'fft_len 1200, batch 32, 256 neurons, batch_norm before nonlinearity'}})\n",
      "('1', {'M': 'nn', 'S': '0.9699594736099243', 'D': 'fft_len 1200, batch 128, 256 neurons, batch_norm before nonlinearity', 'P': './Models/Noise_BClassifiers/nn_bin-acc_0.9658.pt', 'E': {'score': {'metric': {'name': 'bin-acc', 'values': [0.9734042286872864, 0.9726443886756897, 0.9721378087997437, 0.9658054709434509, 0.9658054709434509]}, 'loss': {'name': 'bin-cross-entropy', 'values': [0.2739558517932892, 0.2659973204135895, 0.17677325010299683, 0.18252500891685486, 0.11571940034627914]}}, 'data': {'fs': 48000, 'fft_len': 1200, 'duration': 0.05}, 'learning': {'n_epochs': 50, 'n_batches': 128, 'learning_rate': 0.001, 'folds': 5}, 'model_info': {'name': 'nn', 'path': './Models/Noise_BClassifiers/nn_bin-acc_0.9658.pt'}, 'description': 'fft_len 1200, batch 128, 256 neurons, batch_norm before nonlinearity'}})\n"
     ]
    }
   ],
   "source": [
    "n_exp = 3\n",
    "print(*sorted(elb.experiments.items(), key=lambda x: x[1]['S'], reverse=True)[:n_exp], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00014833-9774-466b-ac96-5a78531cb5ed",
   "metadata": {},
   "source": [
    "### Note classifier\n",
    "Just classifies a chunk of audio as one of the notes presented (88+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b824ec45-230d-4efd-bf3d-bd35a0279beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment: 8\n"
     ]
    }
   ],
   "source": [
    "models_path = \"./Models/Note_Classifiers/\"\n",
    "el = ExperimentsLogger(models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a90c16f6-735b-4656-aeae-2532ec8de984",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Epoch: 0 Loss clf: 4.06616751352946 Metrics: (0.4889945089817047, 0.628000020980835)\n",
      "Epoch: 5 Loss clf: 3.9588475505510967 Metrics: (0.6700850129127502, 0.6775000095367432)\n",
      "Epoch: 10 Loss clf: 3.8223320603370667 Metrics: (0.6967233419418335, 0.6940000057220459)\n",
      "Epoch: 15 Loss clf: 3.8033057411511737 Metrics: (0.7134817242622375, 0.7055000066757202)\n",
      "Epoch: 20 Loss clf: 3.7157182455062867 Metrics: (0.7216107845306396, 0.7099999785423279)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 96\u001b[0m\n\u001b[1;32m     94\u001b[0m tr_metric\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     95\u001b[0m val_metric\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainloader):\n\u001b[1;32m     98\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     99\u001b[0m     x \u001b[38;5;241m=\u001b[39m x[:,\u001b[38;5;28;01mNone\u001b[39;00m,:]\n",
      "File \u001b[0;32m~/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[9], line 125\u001b[0m, in \u001b[0;36mAudioDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m    124\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_info\u001b[38;5;241m.\u001b[39mloc[idx]\n\u001b[0;32m--> 125\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfft_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m    128\u001b[0m         y \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:1381\u001b[0m, in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1379\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1381\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m            \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munpack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[0;32m~/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:1021\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     data \u001b[38;5;241m=\u001b[39m _preprocess_comments(data, comments, encoding)\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_dtype_via_object_chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1021\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_filelike\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimaginary_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimaginary_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiplines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilelike\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilelike\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbyte_converters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbyte_converters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;66;03m# This branch reads the file into chunks of object arrays and then\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;66;03m# casts them to the desired actual dtype.  This ensures correct\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;66;03m# string-length and datetime-unit discovery (like `arr.astype()`).\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;66;03m# Due to chunking, certain error reports are less clear, currently.\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filelike:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/codecs.py:331\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.getstate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    328\u001b[0m     IncrementalDecoder\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgetstate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;66;03m# additional state info is always 0\u001b[39;00m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msetstate\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;66;03m# ignore additional state info\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from torch import nn\n",
    "\n",
    "rs = 42\n",
    "torch.manual_seed(rs)\n",
    "np.random.seed(rs)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        self.n_neurons = 256\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(n_input, self.n_neurons), # new 128 32\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm1d(self.n_neurons),\n",
    "            # nn.Linear(512, 256), # new 128 32\n",
    "            # nn.Tanh(),\n",
    "            # nn.BatchNorm1d(256),\n",
    "            # nn.Linear(128, 128),\n",
    "            # nn.Tanh(),\n",
    "            nn.Linear(self.n_neurons, n_output),\n",
    "            nn.Softmax(dim=1))\n",
    "        # self.model.apply(self.init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    # def validate(self, validation_set, batch_size, loss_function):\n",
    "    #     #  creating a list to hold loss per batch\n",
    "    #     loss_per_batch = []\n",
    "        \n",
    "    #     #  defining model state\n",
    "    #     network.eval()\n",
    "        \n",
    "    #     #  defining dataloader\n",
    "    #     val_loader = DataLoader(validation_set, batch_size)\n",
    "\n",
    "device = \"\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "\n",
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "batch_size = 64\n",
    "batch_n = int(ds.__len__()/batch_size)\n",
    "\n",
    "lr = 0.001\n",
    "num_epochs = 200 # 160 80 # 20\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "tr_metric = MulticlassAccuracy()\n",
    "val_metric = MulticlassAccuracy()\n",
    "mclist = []\n",
    "losslist = []\n",
    "\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(ds)):\n",
    "    \n",
    "    # Print\n",
    "    print(f'\\nFOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                      ds, \n",
    "                      batch_size=batch_size, sampler=train_subsampler, pin_memory=True)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "                      ds,\n",
    "                      batch_size=batch_size, sampler=test_subsampler, pin_memory=True)\n",
    "\n",
    "    clf = Classifier(ds.fft_len, ds.n_classes).to(device=device)\n",
    "    optimizer = torch.optim.Adam(clf.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        clf.train()\n",
    "        tr_metric.reset()\n",
    "        val_metric.reset()\n",
    "        for n, (x, y) in enumerate(trainloader):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            x = x[:,None,:]\n",
    "            prediction = clf(x.to(torch.float32).cuda()) # .to(torch.float)\n",
    "            loss_classifier = loss_function(prediction, y.cuda())\n",
    "            loss_classifier.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tr_metric.update(prediction, y.argmax(axis=1))\n",
    "\n",
    "        clf.eval()\n",
    "        for n, (x, y) in enumerate(testloader):\n",
    "            x = x[:,None,:]\n",
    "            prediction = clf(x.to(torch.float32).cuda())\n",
    "            val_metric.update(prediction, y.argmax(axis=1))\n",
    "        mcv = val_metric.compute()\n",
    "        \n",
    "        if epoch % 5 == 0 or epoch == (num_epochs-1):\n",
    "            mct = tr_metric.compute()\n",
    "            print(f\"Epoch: {epoch} Loss clf: {loss_classifier} Metrics: ({mct}, {mcv})\")\n",
    "    \n",
    "    mclist.append(float(mcv))\n",
    "    losslist.append(float(loss_classifier))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d878b5ef-f266-4f62-b561-25c5dc45afe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as ./Models/nn_mult-acc_0.7609.pt\n",
      "{'score': {'metric': {'name': 'mult-acc', 'values': [0.7860000133514404, 0.7418709397315979, 0.7633817195892334, 0.7623811960220337, 0.7608804106712341]}, 'loss': {'name': 'cross-entropy', 'values': [3.663506273428599, 3.738224451659156, 3.6014773024887337, 3.7023925155889796, 3.787837989994737]}}, 'data': {'fs': 48000, 'fft_len': 1200, 'duration': 0.05}, 'learning': {'n_epochs': 200, 'n_batches': 64, 'learning_rate': 0.001, 'folds': 5}, 'model_info': {'name': 'nn', 'path': './Models/nn_mult-acc_0.7609.pt'}, 'description': 'fft_len 1200, batch 64, 256 neurons, batch_norm before nonlinearity'}\n",
      "\n",
      "Avg cross-val metric: 0.7629028558731079, STD: 0.01400294186387866\n",
      "Experiment was written successfully\n"
     ]
    }
   ],
   "source": [
    "# model_name, score, params, save=True\n",
    "# mclist = [float(mc) for mc in mclist]\n",
    "# losslist = [float(loss) for loss in losslist]\n",
    "\n",
    "metric_name = \"mult-acc\"\n",
    "loss_name = \"cross-entropy\"\n",
    "model_name = \"nn\"\n",
    "description = f\"fft_len {ds.fft_len}, batch {batch_size}, {clf.n_neurons} neurons, batch_norm before nonlinearity\"\n",
    "\n",
    "\n",
    "model_path = f\"{models_path}{model_name}_{metric_name}_{mclist[-1]:.4f}.pt\"\n",
    "model_scripted = torch.jit.script(clf) # Export to TorchScript\n",
    "model_scripted.save(model_path) # Save\n",
    "\n",
    "print(f\"Saved as {model_path}\")\n",
    "\n",
    "exp_info = {\"score\":{\"metric\":{\"name\":metric_name, \"values\":mclist}, \"loss\":{\"name\":loss_name, \"values\":losslist}},\n",
    "            \"data\": {\"fs\": ds.w_rw.Fs, \"fft_len\":ds.fft_len, \"duration\":(ds.fft_len*2)/ds.w_rw.Fs},\n",
    "            \"learning\":{\"n_epochs\":num_epochs, \"n_batches\":batch_size, \"learning_rate\":lr, \"folds\":k_folds},\n",
    "            \"model_info\":{\"name\":model_name, \"path\":model_path},\n",
    "            \"description\": description}\n",
    "\n",
    "print(exp_info)\n",
    "el.write_experiment(exp_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb1922-dc87-4a77-9992-0c7f2fce453b",
   "metadata": {},
   "source": [
    "#### Get description of the best experiments so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "069952f7-f122-4cb8-acbd-69d0b744f4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('5', {'M': 'nn', 'S': '0.7741090059280396', 'D': 'fft_len 1200, batch 32, 64 neurons, batch_norm', 'P': './Models/nn_mult-acc_0.7729.pt', 'E': {'score': {'metric': {'name': 'mult-acc', 'values': [0.7804999947547913, 0.756378173828125, 0.7848924398422241, 0.7758879661560059, 0.7728864550590515]}, 'loss': {'name': 'cross-entropy', 'values': [3.699983750070844, 3.8210244918691703, 3.848136104386428, 3.714581719760237, 3.680334946204876]}}, 'data': {'fs': 48000, 'fft_len': 1200, 'duration': 0.05}, 'learning': {'n_epochs': 200, 'n_batches': 32, 'learning_rate': 0.001, 'folds': 5}, 'model_info': {'name': 'nn', 'path': './Models/nn_mult-acc_0.7729.pt'}, 'description': 'fft_len 1200, batch 32, 64 neurons, batch_norm'}})\n",
      "('3', {'M': 'nn', 'S': '0.7697069525718689', 'D': 'fft_len 1200, batch 32, 256 neurons, batch_norm', 'P': './Models/nn_mult-acc_0.7784.pt', 'E': {'score': {'metric': {'name': 'mult-acc', 'values': [0.7789999842643738, 0.7483741641044617, 0.780390202999115, 0.7623811960220337, 0.7783892154693604]}, 'loss': {'name': 'cross-entropy', 'values': [3.720505475997925, 3.85259354525599, 3.5422544068303603, 3.6917205103512467, 3.7878223205434867]}}, 'data': {'fs': 48000, 'fft_len': 1200, 'duration': 0.05}, 'learning': {'n_epochs': 200, 'n_batches': 32, 'learning_rate': 0.001, 'folds': 5}, 'model_info': {'name': 'nn', 'path': './Models/nn_mult-acc_0.7784.pt'}, 'description': 'fft_len 1200, batch 32, 256 neurons, batch_norm'}})\n",
      "('6', {'M': 'nn', 'S': '0.7697069525718689', 'D': 'fft_len 1200, batch 32, 256 neurons, batch_norm', 'P': './Models/nn_mult-acc_0.7784.pt', 'E': {'score': {'metric': {'name': 'mult-acc', 'values': [0.7789999842643738, 0.7483741641044617, 0.780390202999115, 0.7623811960220337, 0.7783892154693604]}, 'loss': {'name': 'cross-entropy', 'values': [3.720505475997925, 3.85259354525599, 3.5422544068303603, 3.6917205103512467, 3.7878223205434867]}}, 'data': {'fs': 48000, 'fft_len': 1200, 'duration': 0.05}, 'learning': {'n_epochs': 200, 'n_batches': 32, 'learning_rate': 0.001, 'folds': 5}, 'model_info': {'name': 'nn', 'path': './Models/nn_mult-acc_0.7784.pt'}, 'description': 'fft_len 1200, batch 32, 256 neurons, batch_norm'}})\n",
      "('7', {'M': 'nn', 'S': '0.7629028558731079', 'D': 'fft_len 1200, batch 64, 256 neurons, batch_norm before nonlinearity', 'P': './Models/nn_mult-acc_0.7609.pt', 'E': {'score': {'metric': {'name': 'mult-acc', 'values': [0.7860000133514404, 0.7418709397315979, 0.7633817195892334, 0.7623811960220337, 0.7608804106712341]}, 'loss': {'name': 'cross-entropy', 'values': [3.663506273428599, 3.738224451659156, 3.6014773024887337, 3.7023925155889796, 3.787837989994737]}}, 'data': {'fs': 48000, 'fft_len': 1200, 'duration': 0.05}, 'learning': {'n_epochs': 200, 'n_batches': 64, 'learning_rate': 0.001, 'folds': 5}, 'model_info': {'name': 'nn', 'path': './Models/nn_mult-acc_0.7609.pt'}, 'description': 'fft_len 1200, batch 64, 256 neurons, batch_norm before nonlinearity'}})\n",
      "('1', {'M': 'nn', 'S': '0.759865140914917', 'D': 'batch 32, 256 neurons, batch_norm', 'P': './Models/nn_mult-acc_0.7408.pt', 'E': {'score': {'metric': {'name': 'mult-acc', 'values': [0.7816677093505859, 0.7396562695503235, 0.7619350552558899, 0.7753023505210876, 0.740764319896698]}, 'loss': {'name': 'cross-entropy', 'values': [3.760151581330733, 3.780729033730247, 3.6895772977308794, 3.689006198536266, 3.757690966129303]}}, 'data': {'fs': 48000, 'fft_len': 1500, 'duration': 0.0625}, 'learning': {'n_epochs': 160, 'n_batches': 32, 'learning_rate': 0.001, 'folds': 5}, 'model_info': {'name': 'nn', 'path': './Models/nn_mult-acc_0.7408.pt'}, 'description': 'batch 32, 256 neurons, batch_norm'}})\n"
     ]
    }
   ],
   "source": [
    "n_exp = 3\n",
    "print(*sorted(el.experiments.items(), key=lambda x: x[1]['S'], reverse=True)[:n_exp], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad1e572-e47e-4fe3-97a4-f9158bcb3cca",
   "metadata": {},
   "source": [
    "## Test IRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7799a5cf-7342-4d4e-bec6-df996a387baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=Classifier\n",
       "  (model): RecursiveScriptModule(\n",
       "    original_name=Sequential\n",
       "    (0): RecursiveScriptModule(original_name=Flatten)\n",
       "    (1): RecursiveScriptModule(original_name=Linear)\n",
       "    (2): RecursiveScriptModule(original_name=Tanh)\n",
       "    (3): RecursiveScriptModule(original_name=BatchNorm1d)\n",
       "    (4): RecursiveScriptModule(original_name=Linear)\n",
       "    (5): RecursiveScriptModule(original_name=Softmax)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "r_len = 2400\n",
    "\n",
    "bmodel_path = 'Models/Noise_BClassifiers/nn_bin-acc_0.9749.pt'\n",
    "bmodel = torch.jit.load(bmodel_path).cuda()\n",
    "bmodel.eval()\n",
    "\n",
    "model_path = 'Models/Note_Classifiers/nn_mult-acc_0.7784.pt'\n",
    "\n",
    "model = torch.jit.load(model_path).cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487860c3-c2b4-4f0d-9ef7-0b2138e0befa",
   "metadata": {},
   "source": [
    "### Live-time testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57852b5c-74be-4f88-8e71-24441b799e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key  0 plays with prob 1.00, top 3 probs [1.00 0.00 0.00]\n",
      "All streams closed\n"
     ]
    }
   ],
   "source": [
    "from pynput import keyboard\n",
    "\n",
    "import pyaudio\n",
    "import sounddevice as sd\n",
    "import wave\n",
    "from scipy.io.wavfile import read\n",
    "from scipy.signal import spectrogram\n",
    "from scipy.signal import welch\n",
    "\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from IPython.display import clear_output\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "float_formatter = \"{:.2f}\".format\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})\n",
    "\n",
    "device = sd.query_devices(device=\"pipewire\", kind=None)[\"index\"]\n",
    "\n",
    "FORMAT = pyaudio.paInt16  #   (16 , )\n",
    "CHANNELS = 1\n",
    "RATE = 48000  #  \n",
    "CHUNK_IN = r_len\n",
    "DEVICE_IDS = (device, device)\n",
    "\n",
    "e = 10**(-10)\n",
    "\n",
    "def setup_streams(form=FORMAT, chan=CHANNELS, \n",
    "                  chunk_in=CHUNK_IN, Fs=RATE,\n",
    "                dev_id = DEVICE_IDS):\n",
    "    \n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    i_stream = p.open(format=form,\n",
    "                channels=chan,\n",
    "                rate=Fs,\n",
    "                input=True,\n",
    "                frames_per_buffer=chunk_in,\n",
    "                input_device_index=dev_id[0])\n",
    "    i_stream.stop_stream()\n",
    "    return p, i_stream\n",
    "\n",
    "def close_streams(p, i_stream):\n",
    "    i_stream.close()\n",
    "    p.terminate()\n",
    "    print(\"All streams closed\")\n",
    "\n",
    "def getfft(sig, Fs):\n",
    "    nps = len(sig)-1\n",
    "    f,fts = welch(sig, window = 'boxcar', fs = Fs, nperseg = nps, \n",
    "                  noverlap = 0, nfft = nps, scaling = \"density\")\n",
    "    return f, fts\n",
    "\n",
    "def predict(sequence, thresh, Fs=RATE):\n",
    "    sequence = np.frombuffer(sequence, dtype=np.int16)\n",
    "    \n",
    "    _, x = getfft(sequence, Fs)\n",
    "    x = torch.from_numpy(x[None, None,:])\n",
    "    x = x.to(torch.float32).cuda()\n",
    "    \n",
    "    note = 0\n",
    "    nprob = 1\n",
    "    prediction = torch.zeros([89])\n",
    "    prediction[0] = 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        isnote = bmodel(x)\n",
    "        if isnote > thresh:\n",
    "            prediction = model(x)\n",
    "            # print(prediction)\n",
    "            note = np.array(prediction.cpu()).argmax(axis=1).astype(int)[0]\n",
    "            nprob = float(prediction[0][note])\n",
    "    return note, nprob, prediction\n",
    "\n",
    "proceed = True\n",
    "\n",
    "def on_press(key):\n",
    "    global proceed\n",
    "    if key == keyboard.Key.esc:\n",
    "        proceed = False\n",
    "\n",
    "def listen_to_music(thresh1=0.7, thresh2=0, chunk_in=CHUNK_IN):\n",
    "    global proceed\n",
    "    n_top = 3\n",
    "    try:\n",
    "        pa, i_str = setup_streams()\n",
    "        listener = keyboard.Listener(on_press=on_press)\n",
    "        listener.start()\n",
    "        # listener.join()\n",
    "        model.eval()\n",
    "        bmodel.eval()\n",
    "        note = 0\n",
    "        prob = 1\n",
    "        proceed_choice = input(\"Press Enter to start recording...\")\n",
    "        i_str.start_stream()\n",
    "        while proceed:  \n",
    "            frame = i_str.read(chunk_in)\n",
    "            results = predict(frame, thresh1)\n",
    "            if results[1] > thresh2:\n",
    "                note, prob, prediction = results\n",
    "            clear_output(wait=True)\n",
    "            key = str(note)\n",
    "            key = (2-len(key))*\" \"+key\n",
    "            print(f\"Key {key} plays with prob {prob:.2f}, top {n_top} probs\", \n",
    "                  np.array(torch.topk(prediction.flatten(), n_top).values.cpu()))  \n",
    "        close_streams(pa, i_str)\n",
    "    except Exception as err:\n",
    "        print(traceback.format_exc())\n",
    "        print(err)\n",
    "        close_streams(pa, i_str)\n",
    "        \n",
    "listen_to_music()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a01e6a1-cd1b-4efd-88c2-e6b3f8c690aa",
   "metadata": {},
   "source": [
    "### Iterative testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d351d0-567f-442f-adbf-529cd5609975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom oscilloscope (yep, again)\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.fft import fft\n",
    "from scipy.signal import stft\n",
    "from scipy.signal import welch\n",
    "from scipy.signal import resample\n",
    "from scipy.signal import square\n",
    "from scipy.signal import butter, freqs, sosfilt\n",
    "\n",
    "class oscope:\n",
    "    def __init__(self, seg, nol, nff):\n",
    "        self.seg = seg\n",
    "        self.nol = nol\n",
    "        self.nff = nff\n",
    "        \n",
    "    def setp(self, seg, nol, nff):\n",
    "        self.seg = seg\n",
    "        self.nol = nol\n",
    "        self.nff = nff\n",
    "        \n",
    "    def tplot(self, t, sig, N):\n",
    "        plt.figure(figsize = (6,3))\n",
    "        plt.plot(t[:N], sig[:N])\n",
    "        plt.title(\"Oscillogram\")\n",
    "        plt.xlabel(\"Time, s\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "    def splot(self, f, fts, N):\n",
    "        plt.figure(figsize = (6,3))\n",
    "        plt.plot(f[:N], fts[:N], color = 'purple')\n",
    "        plt.title(\"Spectrum\")\n",
    "        plt.xlabel(\"Frequency, Hz\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    \n",
    "    def tsplot(self, sig, Fs):\n",
    "        f,fts = welch(sig, window = 'boxcar', fs = Fs, nperseg = self.seg, \n",
    "                      noverlap = self.nol, nfft = self.nff, scaling = \"density\")\n",
    "        fts = np.sqrt(fts*(Fs*2/seg))\n",
    "        self.splot(f, fts, -1)\n",
    "        \n",
    "    def getmax(self, sig, Fs):\n",
    "        f,fts = welch(sig, window = 'boxcar', fs = Fs, nperseg = self.seg, \n",
    "                      noverlap = self.nol, nfft = self.nff, scaling = \"density\")\n",
    "        return f[np.argmax(fts)]\n",
    "\n",
    "    def getfft(self, sig, Fs):\n",
    "        f,fts = welch(sig, window = 'boxcar', fs = Fs, nperseg = self.seg, \n",
    "                      noverlap = self.nol, nfft = self.nff, scaling = \"density\")\n",
    "        return f, fts\n",
    "\n",
    "l = 10000\n",
    "\n",
    "seg = int(l/2)-1 # 4999\n",
    "nol = 0\n",
    "nff = None\n",
    "osc = oscope(seg, nol, nff)\n",
    "# l = 10000\n",
    "# shift = 20000\n",
    "# osc.tsplot(sequence[shift:shift+l], sample_rate)\n",
    "# osc.getmax(sequence[shift:shift+l], sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67fbeb4b-ed50-4176-ad7f-a56f5884e0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All streams closed\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import sounddevice as sd\n",
    "import wave\n",
    "from scipy.io.wavfile import read\n",
    "from scipy.signal import spectrogram\n",
    "\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from IPython.display import clear_output\n",
    "import traceback\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = sd.query_devices(device=\"pipewire\", kind=None)[\"index\"]\n",
    "\n",
    "FORMAT = pyaudio.paInt16  #   (16 , )\n",
    "CHANNELS = 1\n",
    "RATE = 48000  #  \n",
    "CHUNK_IN = 512\n",
    "CHUNK_OUT = r_len\n",
    "DEVICE_IDS = (device, device)\n",
    "\n",
    "e = 10**(-10)\n",
    "\n",
    "def to_wav(path, frames, p, form=FORMAT, ch=CHANNELS, Fs=RATE):\n",
    "    wf = wave.open(path, \"wb\")\n",
    "    wf.setsampwidth(p.get_sample_size(form))\n",
    "    wf.setnchannels(ch)\n",
    "    wf.setframerate(Fs)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "    print(f\"Saved to {path}\")\n",
    "\n",
    "def record(stream, duration, chunk_in = CHUNK_IN, Fs=RATE):\n",
    "    \n",
    "    frames = []\n",
    "    stream.start_stream()\n",
    "    for i in range(int(Fs / chunk_in * duration)):\n",
    "        data = stream.read(chunk_in)\n",
    "        frames.append(data)\n",
    "    stream.stop_stream()\n",
    "    # stream.close()\n",
    "    # print(len(frames))\n",
    "    return frames\n",
    "\n",
    "def visualize(file_path, Fs=RATE):\n",
    "    file = read(file_path)\n",
    "    sequence = file[1]\n",
    "\n",
    "    f, t, Sxx = spectrogram(sequence, Fs)\n",
    "    plt.pcolormesh(t, f, np.log(Sxx+e), shading='gouraud')\n",
    "    plt.ylabel('Frequency [Hz]')\n",
    "    plt.xlabel('Time [sec]')\n",
    "    plt.show()\n",
    "\n",
    "def visualize(file_path, Fs=RATE, os=osc):\n",
    "    file = read(file_path)\n",
    "    sequence = file[1]\n",
    "    os.tsplot(sequence, Fs)\n",
    "    print(os.getmax(sequence, Fs))\n",
    "    \n",
    "def play(stream, file_path, chunk_out=CHUNK_OUT):\n",
    "    wf = wave.open(file_path, 'rb')\n",
    "    # read data (based on the chunk size)\n",
    "    data = wf.readframes(chunk_out)\n",
    "    # play stream (looping from beginning of file to the end)\n",
    "    stream.start_stream()\n",
    "    while data:\n",
    "        # writing to the stream is what *actually* plays the sound.\n",
    "        stream.write(data)\n",
    "        data = wf.readframes(chunk_out)\n",
    "\n",
    "    stream.stop_stream()\n",
    "    # stream.stop_stream()\n",
    "    wf.close()\n",
    "    # stream.close()    \n",
    "    # p.terminate()\n",
    "\n",
    "def setup_streams(form=FORMAT, chan=CHANNELS, \n",
    "                  chunk_in = CHUNK_IN, Fs=RATE,\n",
    "                  chunk_out=CHUNK_OUT, dev_id = DEVICE_IDS):\n",
    "    \n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    i_stream = p.open(format=form,\n",
    "                channels=chan,\n",
    "                rate=Fs,\n",
    "                input=True,\n",
    "                frames_per_buffer=chunk_in,\n",
    "                input_device_index=dev_id[0])\n",
    "    # open stream based on the wave object which has been input.\n",
    "    # print(chan, dev_id[0], dev_id[1])\n",
    "    o_stream = p.open(format = form,\n",
    "                    channels = chan,\n",
    "                    rate = Fs,\n",
    "                    output = True,\n",
    "                    frames_per_buffer=chunk_out,\n",
    "                    output_device_index=dev_id[1])\n",
    "    i_stream.stop_stream()\n",
    "    o_stream.stop_stream()\n",
    "    return p, i_stream, o_stream\n",
    "\n",
    "def close_streams(p, i_stream, o_stream):\n",
    "    i_stream.close()\n",
    "    o_stream.close()\n",
    "    p.terminate()\n",
    "    print(\"All streams closed\")\n",
    "\n",
    "def getfft(sig, Fs):\n",
    "    nps = len(sig)-1\n",
    "    f,fts = welch(sig, window = 'boxcar', fs = Fs, nperseg = nps, \n",
    "                  noverlap = 0, nfft = nps, scaling = \"density\")\n",
    "    return f, fts\n",
    "\n",
    "def predict(file_path, chunk_out=r_len, Fs=RATE):\n",
    "\n",
    "    wfile = read(file_path)\n",
    "    sequence = wfile[1]\n",
    "    result_df = pd.DataFrame(columns=[\"time\", \"note\", \"prob\"])\n",
    "    for chunk in range(int(len(sequence)/chunk_out)):\n",
    "        subseq=np.array(sequence[chunk*chunk_out:(chunk+1)*chunk_out])\n",
    "        # print(len(subseq),subseq)\n",
    "        _, x = getfft(subseq, Fs)\n",
    "        x = torch.from_numpy(x[None, None,:])\n",
    "        # print(x.shape)\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            prediction = model(x.to(torch.float32).cuda())\n",
    "            note = np.array(prediction.cpu()).argmax(axis=1).astype(int)[0]\n",
    "            nprob = float(prediction.cpu()[0][note])\n",
    "            result_df.loc[len(result_df)] = (chunk*chunk_out/Fs, note, nprob)\n",
    "    print(result_df)\n",
    "  \n",
    "def record_data(folder_path, duration=5):\n",
    "    pa, i_str, o_str = setup_streams()\n",
    "    try:\n",
    "        while True: \n",
    "            proceed_choice = bool(int(input(\"Start a recording?\")))\n",
    "            clear_output()\n",
    "            # time.sleep(1)\n",
    "            if not proceed_choice:\n",
    "                break\n",
    "            \n",
    "            frames = record(i_str, duration)\n",
    "            # print(\"Wow\")\n",
    "            tpath = join(folder_path, \"temp.wav\")\n",
    "            to_wav(tpath, frames, pa)\n",
    "            visualize(tpath)\n",
    "            \n",
    "            # play_choice = bool(int(input(\"Play an audio?\")))\n",
    "            # if play_choice:\n",
    "            predict(tpath)\n",
    "            \n",
    "        close_streams(pa, i_str, o_str)\n",
    "        \n",
    "    except Exception as err:\n",
    "        print(traceback.format_exc())\n",
    "        print(err)\n",
    "        close_streams(pa, i_str, o_str)\n",
    "        \n",
    "mypath = \"./Data/Test/New/\"\n",
    "\n",
    "record_data(mypath, 1.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d900e4a7-9a4a-4b6f-be7a-aae4ad1d0e91",
   "metadata": {},
   "source": [
    "### URL File Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae243fbe-fe3a-4764-ae4b-5ba3fadda552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import sounddevice as sd\n",
    "import wave\n",
    "from scipy.io.wavfile import read\n",
    "from scipy.signal import spectrogram\n",
    "from scipy.signal import welch\n",
    "\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from IPython.display import clear_output\n",
    "import traceback\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import yt_dlp\n",
    "\n",
    "file_path = 'temp'\n",
    "\n",
    "# full options list https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/YoutubeDL.py#L128-L278\n",
    "# shorten version https://stackoverflow.com/questions/38658046/how-can-i-find-all-ydl-opts\n",
    "\n",
    "ydl_opts = {\n",
    "    # 'verbose': False,\n",
    "    'quiet': True,\n",
    "    'format': 'bestaudio/best',\n",
    "    'outtmpl': file_path,\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'wav',\n",
    "    }],\n",
    "}\n",
    "\n",
    "\n",
    "e = 10**(-10)\n",
    "    \n",
    "def getfft(sig, Fs):\n",
    "    nps = len(sig)-1\n",
    "    f,fts = welch(sig, window = 'boxcar', fs = Fs, nperseg = nps, \n",
    "                  noverlap = 0, nfft = nps, scaling = \"density\")\n",
    "    return f, fts\n",
    "\n",
    "def predict(file_path, chunk_out=r_len):\n",
    "\n",
    "    wf = wave.open(file_path, \"rb\")\n",
    "    Fs = wf.getframerate()\n",
    "    wf.close()\n",
    "        \n",
    "    wfile = read(file_path)\n",
    "    sequence = wfile[1]\n",
    "    result_df = pd.DataFrame(columns=[\"time\", \"note\", \"prob\"])\n",
    "    for chunk in range(int(len(sequence)/chunk_out)):\n",
    "        subseq=np.array(sequence[chunk*chunk_out:(chunk+1)*chunk_out])\n",
    "        subseq=subseq[:,0] # first channel\n",
    "        # print(len(subseq),subseq)\n",
    "        _, x = getfft(subseq, Fs)\n",
    "        # print(x.shape)\n",
    "        x = torch.from_numpy(x[None, None,:])\n",
    "        # print(x.shape)\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            # print(x.to(torch.float32).shape) \n",
    "            prediction = model(x.to(torch.float32).cuda())\n",
    "            note = np.array(prediction.cpu()).argmax(axis=1).astype(int)[0]\n",
    "            nprob = float(prediction.cpu()[0][note])\n",
    "            result_df.loc[len(result_df)] = (chunk*chunk_out/Fs, note, nprob)\n",
    "    return result_df\n",
    "\n",
    "def download(url):\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download(url)\n",
    "    \n",
    "    \n",
    "def url_predict():\n",
    "    try:\n",
    "        while True: \n",
    "            url = input(\"Paste url here\")\n",
    "            clear_output()\n",
    "            if url == \"\":\n",
    "                break\n",
    "                \n",
    "            print(\"Download in progress...\")\n",
    "            download(url)\n",
    "            print(\"Done\")\n",
    "            tpath = file_path+\".wav\"\n",
    "            result = predict(tpath)\n",
    "            result.to_csv('predictions.csv', index=False)\n",
    "            \n",
    "        # close_streams(pa, i_str, o_str)\n",
    "        \n",
    "    except Exception as err:\n",
    "        print(traceback.format_exc())\n",
    "        print(err)\n",
    "        # close_streams(pa, i_str, o_str)\n",
    "\n",
    "# url = \"https://youtu.be/2HZnTNIN648?si=TVziVpuXmDIF3cfu\"\n",
    "\n",
    "url_predict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e2d9a0-84f2-49d2-8135-0cf41349aeab",
   "metadata": {},
   "source": [
    "#### The end! Hurraaaay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26683b1-22c6-4a7b-b7a9-0fe05b8f2c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7a3c680-3e48-4e32-a28f-da1ae3e46f14",
   "metadata": {},
   "source": [
    "### Some code testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b533c5e-7d5f-4d83-ae70-1624e355a4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pysynth\n",
      "  Downloading pysynth-0.0.4-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: numpy in /home/mephodius/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages (from pysynth) (2.0.2)\n",
      "Requirement already satisfied: pandas in /home/mephodius/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages (from pysynth) (2.2.3)\n",
      "Requirement already satisfied: scipy in /home/mephodius/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages (from pysynth) (1.15.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /home/mephodius/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages (from pysynth) (1.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/mephodius/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages (from scikit-learn>=0.22->pysynth) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/mephodius/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages (from scikit-learn>=0.22->pysynth) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/mephodius/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages (from pandas->pysynth) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/mephodius/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages (from pandas->pysynth) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/mephodius/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages (from pandas->pysynth) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/mephodius/Programming/PythonProjects/DataScience/Pytorch/pytorch/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->pysynth) (1.17.0)\n",
      "Downloading pysynth-0.0.4-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: pysynth\n",
      "Successfully installed pysynth-0.0.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/home/mephodius/Programming/PythonProjects/DataScience/Pytorch/pytorch/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pysynth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9e3f02f-beb5-4804-8fad-98ff36957d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://youtu.be/2HZnTNIN648?si=TVziVpuXmDIF3cfu\n",
      "[youtube] 2HZnTNIN648: Downloading webpage\n",
      "[youtube] 2HZnTNIN648: Downloading tv client config\n",
      "[youtube] 2HZnTNIN648: Downloading player b191cf34\n",
      "[youtube] 2HZnTNIN648: Downloading tv player API JSON\n",
      "[youtube] 2HZnTNIN648: Downloading ios player API JSON\n",
      "[youtube] 2HZnTNIN648: Downloading m3u8 information\n",
      "[info] 2HZnTNIN648: Downloading 1 format(s): 251\n",
      "[download] Destination: temp\n",
      "[download] 100% of    1.31MiB in 00:00:00 at 4.04MiB/s   \n",
      "[ExtractAudio] Destination: temp.wav\n",
      "Deleting original file temp (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "# import youtube_dl\n",
    "import yt_dlp\n",
    "import ffmpeg\n",
    "import sys\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'outtmpl': 'temp',\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'wav',\n",
    "    }],\n",
    "}\n",
    "\n",
    "# def download_from_url(url):\n",
    "#     ydl.download(url)\n",
    "#     stream = ffmpeg.input('output.m4a')\n",
    "#     stream = ffmpeg.output(stream, 'output.wav')\n",
    "\n",
    "\n",
    "# with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "#     ydl.download([url])\n",
    "\n",
    "url = \"https://youtu.be/2HZnTNIN648?si=TVziVpuXmDIF3cfu\"\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download(url)\n",
    "    # download_from_url(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b63acd0b-7266-4a33-bc83-9ba4ca335926",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pysynth' has no attribute 'make_wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpysynth\u001b[39;00m\n\u001b[1;32m      2\u001b[0m test \u001b[38;5;241m=\u001b[39m ( (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m4\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m4\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m4\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m) )\n\u001b[0;32m----> 3\u001b[0m \u001b[43mpysynth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_wav\u001b[49m(test, fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pysynth' has no attribute 'make_wav'"
     ]
    }
   ],
   "source": [
    "import pysynth\n",
    "test = ( ('c', 4), ('e', 4), ('g', 4), ('c5', 1) )\n",
    "pysynth.make_wav(test, fn = \"test.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13dbc50-2cb0-47b4-a4bf-7cd2513c307c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
